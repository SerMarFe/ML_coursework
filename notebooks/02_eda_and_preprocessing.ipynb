{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa54d3b5",
   "metadata": {},
   "source": [
    "# Task 2 - Data Exploration, Analysis, and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1131a0b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "This notebook covers data quality, integration, comprehensive exploration, and preparing the data for the modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d1054f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2.1 Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the two primary datasets (Power Generation and Sensor Data for each plant)\n",
    "plant1_gen = pd.read_csv('../data/raw/Plant_1_Generation_Data.csv')\n",
    "plant1_weather = pd.read_csv('../data/raw/Plant_1_Weather_Sensor_Data.csv')\n",
    "plant2_gen = pd.read_csv('../data/raw/Plant_2_Generation_Data.csv')\n",
    "plant2_weather = pd.read_csv('../data/raw/Plant_2_Weather_Sensor_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba2856",
   "metadata": {},
   "source": [
    "## 2.2 Data Quality and Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4533d9",
   "metadata": {},
   "source": [
    "### 2.2.1 Data Quality Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63144a6",
   "metadata": {},
   "source": [
    "#### A. General checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c1bfe",
   "metadata": {},
   "source": [
    "##### Data Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99183a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for row duplicates in each dataset\n",
    "print(\"Plant 1 Generation duplicates:\", plant1_gen.duplicated().sum(), f\"({round(plant1_gen.duplicated().sum()/len(plant1_gen)*100, 2)}%)\")\n",
    "print(\"Plant 2 Generation duplicates:\", plant2_gen.duplicated().sum(), f\"({round(plant2_gen.duplicated().sum()/len(plant2_gen)*100, 2)}%)\")\n",
    "print(\"Plant 1 Weather duplicates:\", plant1_weather.duplicated().sum(), f\"({round(plant1_weather.duplicated().sum()/len(plant1_weather)*100, 2)}%)\")\n",
    "print(\"Plant 2 Weather duplicates:\", plant2_weather.duplicated().sum(), f\"({round(plant2_weather.duplicated().sum()/len(plant2_weather)*100, 2)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for 34 Days @ 15-min intervals\n",
    "DAYS = 34\n",
    "READINGS_PER_DAY = 24 * 4 # 96\n",
    "EXPECTED_PER_SENSOR = DAYS * READINGS_PER_DAY\n",
    "\n",
    "def analyse_dataset(df, plant_label=\"Plant 1\", days=DAYS, readings_per_day=READINGS_PER_DAY):\n",
    "    \"\"\"\n",
    "    Analyzes the dataset to compute expected vs actual readings per sensor/inverter.\n",
    "    \"\"\"\n",
    "    inverter_count = df['SOURCE_KEY'].nunique()\n",
    "    expected_readings = days * readings_per_day * inverter_count\n",
    "    actual_readings = len(df)\n",
    "    diff = actual_readings - expected_readings\n",
    "\n",
    "    print(f\"\\n--- {plant_label} Generation Data ---\")\n",
    "    print(f\"1. Unique Inverters identified: {inverter_count}\")\n",
    "    print(f\"2. Expected Readings (Target):  {expected_readings:,}\")\n",
    "    print(f\"   (Calculation: {days} days * {readings_per_day} readings * {inverter_count} inverters)\")\n",
    "    print(f\"3. Actual Readings (Raw):       {actual_readings:,}\")\n",
    "    print(f\"4. Conclusion:                  {diff:+,} Rows\")\n",
    "\n",
    "\n",
    "p1_gen_stats = analyse_dataset(plant1_gen, \"Plant 1\")\n",
    "p2_gen_stats = analyse_dataset(plant2_gen, \"Plant 2\")\n",
    "p1_weather_stats = analyse_dataset(plant1_weather, \"Plant 1 Weather\")\n",
    "p2_weather_stats = analyse_dataset(plant2_weather, \"Plant 2 Weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba560f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Slice and do a visual inspection of the data to understand duplications\n",
    "\n",
    "# # Define \"Primary Keys\" to make a row unique.\n",
    "# key_columns = ['DATE_TIME', 'DC_POWER', 'SOURCE_KEY']\n",
    "\n",
    "# # Find rows with duplicate Keys AND NOT duplicate Data\n",
    "# has_duplicate_keys = plant1_gen.duplicated(subset=key_columns, keep=False)\n",
    "# is_exact_duplicate = plant1_gen.duplicated(keep=False)\n",
    "\n",
    "# # We only want rows where keys match, but data differs\n",
    "# conflict_mask = has_duplicate_keys & ~is_exact_duplicate\n",
    "\n",
    "# # Grab 1 random row to act as search parameter\n",
    "# target_row = plant1_gen[conflict_mask].sample(1)\n",
    "\n",
    "# # Retrieve the target row and its conflicts\n",
    "# result = plant1_gen.merge(target_row[key_columns], on=key_columns)\n",
    "\n",
    "# # Display result\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d525f5",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "- When slicing data, we observe exact time stamps with all variables equal except for \"operating_condition\". Understanding this in detail and cleaning accordingly will be critical for the \"Classification of Operating Conditions\" task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752738ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check data types\n",
    "def get_types_report(df):\n",
    "    \"\"\"Generate a report of pandas dtypes and unique Python types for each column in the DataFrame.\"\"\"\n",
    "    types = {}\n",
    "    for col in df.columns:\n",
    "        # Get unique python types in the column (useful for spotting mixed types)\n",
    "        py_types = df[col].map(lambda x: type(x).__name__).unique().tolist()\n",
    "        types[col] = py_types\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "        'column': list(types.keys()),\n",
    "        'pandas_dtype': [df[col].dtype for col in types.keys()],\n",
    "        'python_types': [types[col] for col in types.keys()]\n",
    "    })\n",
    "    return result_df\n",
    "\n",
    "# Get unique Python types and pandas dtypes for the four datasets\n",
    "print(\"Plant 1 Generation - Data Types Report:\")\n",
    "print(get_types_report(plant1_gen), \"\\n\")\n",
    "\n",
    "print(\"Plant 1 Weather - Data Types Report:\")\n",
    "print(get_types_report(plant1_weather), \"\\n\")\n",
    "\n",
    "print(\"Plant 2 Generation - Data Types Report:\")\n",
    "print(get_types_report(plant2_gen), \"\\n\")\n",
    "\n",
    "print(\"Plant 2 Weather - Data Types Report:\")\n",
    "print(get_types_report(plant2_weather))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1a2fb",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "- Mixed formats: In \"Plant 1 - Generation data\" the variable \"Operating_Condition\" mariable exhibits mixed data types, strings and float. This means there are probably missing values in this column (NaN)\n",
    "\n",
    "- Integers for categorical information: Variable Plant_ID is currently encoded as an integer across, is shown as an integer. However, it serves as a unique categorical identifier rather than a quantitative metric. This distinction must be explicitly noted during modeling to ensure algorithms do not misinterpret the ID as having numerical magnitude or order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5283b8",
   "metadata": {},
   "source": [
    "#### B. Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff3293",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for missing values in the four data sets\n",
    "\n",
    "def get_missing_data_report(df):\n",
    "    \"\"\"Generate a report of missing data percentages for each column in the DataFrame.\"\"\"\n",
    "    missing_data_report = pd.DataFrame({\n",
    "        'Columns': df.columns,\n",
    "        'Missing Values': df.isna().sum().values,\n",
    "        'Percentage Missing': ((df.isna().sum().values / len(df)) * 100).round(2)\n",
    "    })\n",
    "    return missing_data_report\n",
    "\n",
    "# Generate and print missing data report for the datasets\n",
    "print(\"Power Generation 1 - Missing Values Report:\")\n",
    "print(get_missing_data_report(plant1_gen),\"\\n\")\n",
    "print(\"Weather Sensor 1 - Missing Values Report:\")\n",
    "print(get_missing_data_report(plant1_weather))\n",
    "print(\"Power Generation 2 - Missing Values Report:\")\n",
    "print(get_missing_data_report(plant2_gen),\"\\n\")\n",
    "print(\"Weather Sensor 2- Missing Values Report:\")\n",
    "print(get_missing_data_report(plant2_weather))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d84975",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for missing data ranges (rows) in the four data sets\n",
    "# -------------------------------------------------------\n",
    "# Generate a report\n",
    "# -------------------------------------------------------\n",
    "def get_time_gap_report(df):\n",
    "    \"\"\"\n",
    "    Generate a report of missing time intervals in the DataFrame.\n",
    "    Assumes 'DATE_TIME' column exists.\n",
    "    \"\"\"\n",
    "    # Ensure datetime and sort\n",
    "    df = df.copy()\n",
    "    df['DATE_TIME'] = pd.to_datetime(df['DATE_TIME'])\n",
    "    df = df.sort_values('DATE_TIME')\n",
    "    \n",
    "    # Create the expected grid for the intervals (perfect 15 min intervals)\n",
    "    start = df['DATE_TIME'].min()\n",
    "    end = df['DATE_TIME'].max()\n",
    "    expected_range = pd.date_range(start=start, end=end, freq='15min')\n",
    "    \n",
    "    # Calculate statistics\n",
    "    expected_count = len(expected_range)\n",
    "    actual_count = df['DATE_TIME'].nunique()\n",
    "    missing_count = expected_count - actual_count\n",
    "    pct_missing = (missing_count / expected_count) * 100\n",
    "    \n",
    "    # Create Report DataFrame\n",
    "    report = pd.DataFrame({\n",
    "        'Metric': ['Start Time', 'End Time', 'Expected Intervals', 'Actual Intervals', 'Missing Intervals', '% Missing'],\n",
    "        'Value': [start, end, expected_count, actual_count, missing_count, round(pct_missing, 2)]\n",
    "    })\n",
    "    return report\n",
    "\n",
    "# Generate and print time gap reports\n",
    "print(\"Power Generation 1 - Time Gap Report:\")\n",
    "print(get_time_gap_report(plant1_gen),\"\\n\")\n",
    "\n",
    "print(\"Weather Sensor 1 - Time Gap Report:\")\n",
    "print(get_time_gap_report(plant1_weather),\"\\n\")\n",
    "\n",
    "print(\"Power Generation 2 - Time Gap Report:\")\n",
    "print(get_time_gap_report(plant2_gen),\"\\n\")\n",
    "\n",
    "print(\"Weather Sensor 2 - Time Gap Report:\")\n",
    "print(get_time_gap_report(plant2_weather))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f83843e",
   "metadata": {},
   "source": [
    "**Conclusion:** \n",
    "\n",
    "Missing values (NaN):\n",
    "- Plant 1: Generation data exhibits missing values. There are about 2.3% missing values in the Operating_Condition column of the data set. On the contrary, weather data shows 0 missing values (empty cells)\n",
    "- Plant 2: There are no missing values in the generation or weather data.\n",
    "\n",
    "Missing temporal datetimes (entire rows):\n",
    "- Plant 1: Is the only one with a time range different from the other datasets. We suspect the date format is not consistent and it is also the reason why the missing value count is high (around 90%) thus needs to be fixed.\n",
    "- Plant 2: The datetime coverage is nearly perfect, it only misses 5 intervals (0.15%) of information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f1b7d",
   "metadata": {},
   "source": [
    "#### C. Inconsistencies & Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdd173b",
   "metadata": {},
   "source": [
    "##### Date Format Fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33de0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all time columns are datetime objects\n",
    "plant1_weather['DATE_TIME'] = pd.to_datetime(plant1_weather['DATE_TIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "plant2_weather['DATE_TIME'] = pd.to_datetime(plant2_weather['DATE_TIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "plant2_gen['DATE_TIME'] = pd.to_datetime(plant2_gen['DATE_TIME'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The timerange of the data is known to be within May 15 and June 17\n",
    "print(\"Data timerange:\", plant1_weather['DATE_TIME'].min(), \"to\", plant1_weather['DATE_TIME'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef21000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve DATE_TIME format ambiguity in Plant 1 Generation Data\n",
    "# Extract month and day parts from DATE_TIME\n",
    "plant1_gen = plant1_gen.join(plant1_gen['DATE_TIME'].str.extract(r'-(\\d{2})-(\\d{2}) ')).rename(columns={0: 'mid', 1: 'right'})\n",
    "\n",
    "# Determine date format based on extracted parts knowing that the timerange of the data is known to be within May 15 and June 17\n",
    "plant1_gen['date_format'] = ''\n",
    "plant1_gen.loc[(plant1_gen['mid']=='05') & (plant1_gen['right']=='06'), \"date_format\"] = 'dd-mm'\n",
    "plant1_gen.loc[(plant1_gen['mid']=='06') & (plant1_gen['right']=='05'), \"date_format\"] = 'mm-dd'\n",
    "plant1_gen.loc[((plant1_gen['mid']=='05') | (plant1_gen['mid']=='06')) & (plant1_gen['date_format'] == ''), \"date_format\"] = 'mm-dd'\n",
    "plant1_gen.loc[((plant1_gen['right']=='05') | (plant1_gen['right']=='06')) & (plant1_gen['date_format'] == ''), \"date_format\"] = 'dd-mm'\n",
    "\n",
    "# Convert DATE_TIME to datetime using the determined format\n",
    "mask_ddmm = plant1_gen['date_format'] == 'dd-mm'\n",
    "mask_mmdd = plant1_gen['date_format'] == 'mm-dd'\n",
    "plant1_gen.loc[mask_ddmm,\"DATE_TIME\"] = pd.to_datetime(plant1_gen.loc[mask_ddmm,\"DATE_TIME\"], format='%Y-%d-%m %H:%M:%S')\n",
    "plant1_gen.loc[mask_mmdd,\"DATE_TIME\"] = pd.to_datetime(plant1_gen.loc[mask_mmdd,\"DATE_TIME\"], format='%Y-%m-%d %H:%M:%S')\n",
    "plant1_gen['DATE_TIME'] = pd.to_datetime(plant1_gen['DATE_TIME']) # Final conversion to datetime\n",
    "plant1_gen = plant1_gen.drop(columns=['mid', 'right', 'date_format'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and print the time gap report again for Plant 1 Generation Data (after fixing DATE_TIME format)\n",
    "print(\"Power Generation 1 - Time Gap Report:\")\n",
    "print(get_time_gap_report(plant1_gen),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ed34a",
   "metadata": {},
   "source": [
    "With the right date format correction, plant generation 1 presents a 3.25% of missing date intervals (instead of 90%+ before the fix)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c6c35",
   "metadata": {},
   "source": [
    "##### Power Format Fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d31c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Power Generation Plant 1\\n\", plant1_gen[['DC_POWER','AC_POWER']].describe(),\"\\n\")\n",
    "print(\"Power Generation Plant 2\\n\", plant2_gen[['DC_POWER','AC_POWER']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb00f72",
   "metadata": {},
   "source": [
    "From the observation of the DC and AC power generation descriptive statistics, we notice that Plant 1's DC power values are an order of magnitude higher than Plant 2's. To fix this error in the data, we scale down Plant 1's DC power values by a factor of 10 to ensure consistency across both plants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix DC_POWER scale issue in Plant 1 Generation Data\n",
    "plant1_gen['DC_POWER'] = plant1_gen['DC_POWER'] / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad88373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprint descriptive statistics after fixing DC_POWER scale issue\n",
    "print(\"Power Generation Plant 1\\n\", plant1_gen[['DC_POWER','AC_POWER']].describe(),\"\\n\")\n",
    "print(\"Power Generation Plant 2\\n\", plant2_gen[['DC_POWER','AC_POWER']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f871a8c2",
   "metadata": {},
   "source": [
    "Now the power values look consistent between both plants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6926f9",
   "metadata": {},
   "source": [
    "##### Further Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e8cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inconsistencies are checked for different situations for all data sets\n",
    "# Grouping for iteration\n",
    "gen_data = [(\"Plant 1\", plant1_gen), (\"Plant 2\", plant2_gen)]\n",
    "weather_data = [(\"Plant 1\", plant1_weather), (\"Plant 2\", plant2_weather)]\n",
    "\n",
    "\n",
    "## Part A: Generation data\n",
    "print(\"\\n--- POWER GENERATION CHECKS ---\")\n",
    "\n",
    "# CHECK 1: Efficiency violation \n",
    "# Impossible for AC Output > DC Input (inverter efficiency can't be > 100%)\n",
    "\n",
    "print(\"\\n[Gen Check 1] Efficiency violation (AC > DC)\")\n",
    "for name, df in gen_data:\n",
    "    # We allow a tiny buffer (0.1 kW) for sensor timing mismatch\n",
    "    errors = df[df['AC_POWER'] > df['DC_POWER'] + 0.1]\n",
    "    print(f\"  {name}: {len(errors)} rows failed.\")\n",
    "\n",
    "# CHECK 2: Negative power\n",
    "# Solar panels cannot consume power (cannot be negative)\n",
    "print(\"\\n[Gen Check 2] Negative power (AC or DC < 0)\")\n",
    "for name, df in gen_data:\n",
    "    errors = df[(df['AC_POWER'] < 0) | (df['DC_POWER'] < 0)]\n",
    "    print(f\"  {name}: {len(errors)} rows failed.\")\n",
    "\n",
    "\n",
    "## Part B: Weather data\n",
    "\n",
    "print(\"\\n--- WEATHER CHECKS ---\")\n",
    "\n",
    "# CHECK 1: No irradiance at night\n",
    "# Irradiance > 0 when it is dark (10 PM - 4 AM short range assuming its summer)\n",
    "print(\"\\n[Weather Check 1] Night irradiance (Irradiation > 0 at Night)\")\n",
    "for name, df in weather_data:\n",
    "    hour = df['DATE_TIME'].dt.hour\n",
    "    night_mask = (hour >= 22) | (hour < 4)\n",
    "    errors = df[night_mask & (df['IRRADIATION'] > 0)]\n",
    "    print(f\"  {name}: {len(errors)} rows failed.\")\n",
    "\n",
    "# CHECK 2: Panel being hot without irradiance at night\n",
    "# No Sun (0 Irr) but Module is significantly hotter than Ambient\n",
    "print(\"\\n[Weather Check 2] Hot panel at night (Mod Temp > Amb Temp + 5Â°C w/ no Sun)\")\n",
    "for name, df in weather_data:\n",
    "    # If Irradiance is 0, Module shouldn't be hot compared to air\n",
    "    errors = df[(df['IRRADIATION'] == 0) & \n",
    "                (df['MODULE_TEMPERATURE'] > df['AMBIENT_TEMPERATURE'] + 5.0)]\n",
    "    print(f\"  {name}: {len(errors)} rows failed.\")\n",
    "\n",
    "# CHECK 3: Broken sensors\n",
    "# Ambient Temp stays exactly the same for 4 consecutive readings (1 hour). Testing for a extreme case.\n",
    "print(\"\\n[Weather Check 3] Broken sensors (Ambient Temp frozen for 1hr)\")\n",
    "for name, df in weather_data:\n",
    "    df_sorted = df.sort_values(by='DATE_TIME') \n",
    "    \n",
    "    # True if current value equals previous value\n",
    "    is_frozen = df_sorted['AMBIENT_TEMPERATURE'].diff() == 0\n",
    "    \n",
    "    # Rolling sum: if 4 consecutive rows are \"True\", we have a 1-hour freeze\n",
    "    frozen_blocks = is_frozen.rolling(4).sum() == 4\n",
    "    print(f\"  {name}: {frozen_blocks.sum()} frozen sequences found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d338d78",
   "metadata": {},
   "source": [
    "**Conclusion:** \n",
    "\n",
    "Generation data\n",
    "- Plant 1: There are 396 instances (~0.6%) of efficiency violations (system reports that AC output is higher than DC, which is physically imposible).\n",
    "- Plant 2: There are 396 instances (~0.6%) of efficiency violations (system reports that AC output is higher than DC, which is physically imposible).\n",
    "\n",
    "Weather data\n",
    "- Plant 1: Passed all consistency checks\n",
    "- Plant 2: 24 instances (~0.7%) of \"Night Sun\" (Irradiance > 0 at night) This culd be due to the sunrise/sunset exact timings or callibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c7f95",
   "metadata": {},
   "source": [
    "### 2.2.2 Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4833c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOULD WE DROP DUPLICATES??? ask Panagiotis Angeloudis\n",
    "def recalculate_yields(df, drop_dup=True):\n",
    "    df = df.copy()\n",
    "    if drop_dup:\n",
    "        df.drop_duplicates(subset=['DATE_TIME', 'SOURCE_KEY'], inplace=True)\n",
    "    df.sort_values(by=['DATE_TIME', 'SOURCE_KEY'], inplace=True)\n",
    "\n",
    "    # day/month for per-day grouping\n",
    "    df['day'] = df['DATE_TIME'].dt.day\n",
    "    df['month'] = df['DATE_TIME'].dt.month\n",
    "\n",
    "    # DAILY_YIELD: cumulative AC_POWER per SOURCE_KEY per day\n",
    "    df['DAILY_YIELD'] = df.groupby(['SOURCE_KEY', 'month', 'day'])['AC_POWER'].cumsum()\n",
    "\n",
    "    # ensure TOTAL_YIELD column exists for the earliest-yield lookup\n",
    "    if 'TOTAL_YIELD' not in df.columns:\n",
    "        df['TOTAL_YIELD'] = np.nan\n",
    "\n",
    "    # get earliest DATE_TIME per SOURCE_KEY and corresponding TOTAL_YIELD\n",
    "    min_dates = df.groupby('SOURCE_KEY', as_index=False)['DATE_TIME'].min()\n",
    "    first_yield = (min_dates\n",
    "                   .merge(df[['SOURCE_KEY', 'DATE_TIME', 'TOTAL_YIELD']],\n",
    "                          on=['SOURCE_KEY', 'DATE_TIME'],\n",
    "                          how='left')\n",
    "                   [['SOURCE_KEY', 'DATE_TIME', 'TOTAL_YIELD']]\n",
    "                   .drop_duplicates('SOURCE_KEY')\n",
    "                   .rename(columns={'DATE_TIME': 'FIRST_DATE_TIME', 'TOTAL_YIELD': 'FIRST_TOTAL_YIELD'}))\n",
    "\n",
    "    # merge FIRST_TOTAL_YIELD and fill missing with 0\n",
    "    df = df.merge(first_yield[['SOURCE_KEY', 'FIRST_TOTAL_YIELD']], on='SOURCE_KEY', how='left')\n",
    "    df['FIRST_TOTAL_YIELD'] = df['FIRST_TOTAL_YIELD'].fillna(0.0)\n",
    "\n",
    "    # recompute TOTAL_YIELD\n",
    "    df['TOTAL_YIELD'] = df['DAILY_YIELD'] + df['FIRST_TOTAL_YIELD']\n",
    "\n",
    "    # cleanup helper columns\n",
    "    df.drop(columns=['day', 'month'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Apply to both plants and assign back\n",
    "plant1_gen_ = recalculate_yields(plant1_gen)\n",
    "plant2_gen_ = recalculate_yields(plant2_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a94d98",
   "metadata": {},
   "source": [
    "### 2.2.3 Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d2eb89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bc752ad",
   "metadata": {},
   "source": [
    "## 2.3 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16aa47",
   "metadata": {},
   "source": [
    "### 2.3.1 Statistical Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7952bd",
   "metadata": {},
   "source": [
    "### 2.3.2 Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c6200",
   "metadata": {},
   "source": [
    "### 2.3.3 Trend Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897be8f",
   "metadata": {},
   "source": [
    "### 2.3.4 Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba81ad",
   "metadata": {},
   "source": [
    "### 2.3.5 Pattern Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecbe1cd",
   "metadata": {},
   "source": [
    "## 2.4 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7549d692",
   "metadata": {},
   "source": [
    "### 2.4.1 Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4888e79",
   "metadata": {},
   "source": [
    "### 2.4.2 Feature Selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
