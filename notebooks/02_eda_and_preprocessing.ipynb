{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa54d3b5",
   "metadata": {},
   "source": [
    "# Task 2 - Data Exploration, Analysis, and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1131a0b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "This notebook covers data quality, integration, comprehensive exploration, and preparing the data for the modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d1054f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2.1 Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cc14d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the two primary datasets (Power Generation and Sensor Data for each plant)\n",
    "plant1_gen = pd.read_csv('Plant_1_Generation_Data.csv')\n",
    "plant1_weather = pd.read_csv('Plant_1_Weather_Sensor_Data.csv')\n",
    "plant2_gen = pd.read_csv('Plant_2_Generation_Data.csv')\n",
    "plant2_weather = pd.read_csv('Plant_2_Weather_Sensor_Data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba2856",
   "metadata": {},
   "source": [
    "## 2.2 Data Quality and Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4533d9",
   "metadata": {},
   "source": [
    "### 2.2.1 Data Quality Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63144a6",
   "metadata": {},
   "source": [
    "#### A. General checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0e3e0e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Plant 1 Generation Data ---\n",
      "1. Unique Inverters identified: 22\n",
      "2. Expected Readings (Target):  71,808\n",
      "   (Calculation: 34 days * 96 readings * 22 inverters)\n",
      "3. Actual Readings (Raw):       1,021,186\n",
      "4. Conclusion:                  +949,378 Rows\n",
      "\n",
      "--- Plant 2 Generation Data ---\n",
      "1. Unique Inverters identified: 22\n",
      "2. Expected Readings (Target):  71,808\n",
      "3. Actual Readings (Raw):       1,421,196\n",
      "4. Conclusion:                  +1,349,388 Rows\n",
      "=== WEATHER DATA SHAPE CHECK ===\n",
      "\n",
      "--- Plant 1 Weather Data ---\n",
      "1. Unique Sensors identified:   1\n",
      "2. Expected Readings (Target):  3,264\n",
      "3. Actual Readings (Raw):       3,182\n",
      "4. Conclusion:                  -82 Rows \n",
      "\n",
      "--- Plant 2 Weather Data ---\n",
      "1. Unique Sensors identified:   1\n",
      "2. Expected Readings (Target):  3,264\n",
      "3. Actual Readings (Raw):       3,259\n",
      "4. Conclusion:                  -5 Rows \n"
     ]
    }
   ],
   "source": [
    "## Overall data shape check \n",
    "\n",
    "# Generation data\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Plant 1 \n",
    "print(\"\\n--- Plant 1 Generation Data ---\")\n",
    "\n",
    "# Identify how many unique inverters there are\n",
    "p1_inverters = plant1_gen['SOURCE_KEY'].unique()\n",
    "p1_inverter_count = len(p1_inverters)\n",
    "print(f\"1. Unique Inverters identified: {p1_inverter_count}\")\n",
    "\n",
    "# Define how many readings are needed for 34 days, 15 minutes time-span\n",
    "# Formula: 34 days * 24 hours * 4 intervals (15 min) * Number of Inverters\n",
    "DAYS = 34\n",
    "READINGS_PER_DAY = 24 * 4 # 96 readings\n",
    "p1_expected_readings = DAYS * READINGS_PER_DAY * p1_inverter_count\n",
    "\n",
    "print(f\"2. Expected Readings (Target):  {p1_expected_readings:,}\")\n",
    "print(f\"   (Calculation: {DAYS} days * {READINGS_PER_DAY} readings * {p1_inverter_count} inverters)\")\n",
    "\n",
    "# Check how many actual readings\n",
    "p1_actual_readings = len(plant1_gen)\n",
    "print(f\"3. Actual Readings (Raw):       {p1_actual_readings:,}\")\n",
    "\n",
    "# Comparison\n",
    "diff_p1 = p1_actual_readings - p1_expected_readings\n",
    "print(f\"4. Conclusion:                  {diff_p1:+,} Rows\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Plant 2 Analysis\n",
    "print(\"\\n--- Plant 2 Generation Data ---\")\n",
    "\n",
    "# Identify how many unique inverters there are\n",
    "p2_inverters = plant2_gen['SOURCE_KEY'].unique()\n",
    "p2_inverter_count = len(p2_inverters)\n",
    "print(f\"1. Unique Inverters identified: {p2_inverter_count}\")\n",
    "\n",
    "# Define how many readings are needed for 34 days, 15 minutes time-span\n",
    "p2_expected_readings = DAYS * READINGS_PER_DAY * p2_inverter_count\n",
    "print(f\"2. Expected Readings (Target):  {p2_expected_readings:,}\")\n",
    "\n",
    "# Check how many actual readings\n",
    "p2_actual_readings = len(plant2_gen)\n",
    "print(f\"3. Actual Readings (Raw):       {p2_actual_readings:,}\")\n",
    "\n",
    "# Comparison\n",
    "diff_p2 = p2_actual_readings - p2_expected_readings\n",
    "print(f\"4. Conclusion:                  {diff_p2:+,} Rows\")\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "# Weather data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== WEATHER DATA SHAPE CHECK ===\")\n",
    "\n",
    "# Load Data\n",
    "p1_weather = pd.read_csv('Plant_1_Weather_Sensor_Data.csv')\n",
    "p2_weather = pd.read_csv('Plant_2_Weather_Sensor_Data.csv')\n",
    "\n",
    "# Constants for 34 Days @ 15-min intervals\n",
    "DAYS = 34\n",
    "READINGS_PER_DAY = 24 * 4 # 96\n",
    "EXPECTED_PER_SENSOR = DAYS * READINGS_PER_DAY\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Plant 1 Analysis\n",
    "\n",
    "print(\"\\n--- Plant 1 Weather Data ---\")\n",
    "\n",
    "# 1. Identify Unique Sensors\n",
    "p1_sensors = p1_weather['SOURCE_KEY'].unique()\n",
    "p1_sensor_count = len(p1_sensors)\n",
    "print(f\"1. Unique Sensors identified:   {p1_sensor_count}\")\n",
    "\n",
    "# 2. Expected Readings\n",
    "p1_expected = EXPECTED_PER_SENSOR * p1_sensor_count\n",
    "print(f\"2. Expected Readings (Target):  {p1_expected:,}\")\n",
    "\n",
    "# 3. Actual Readings\n",
    "p1_actual = len(p1_weather)\n",
    "print(f\"3. Actual Readings (Raw):       {p1_actual:,}\")\n",
    "\n",
    "# 4. Conclusion\n",
    "diff_p1 = p1_actual - p1_expected\n",
    "if diff_p1 == 0:\n",
    "    print(f\"4. Conclusion:                  Exact Match (Perfect Data)\")\n",
    "else:\n",
    "    print(f\"4. Conclusion:                  {diff_p1:+,} Rows \")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Plant 2 Analysis\n",
    "\n",
    "print(\"\\n--- Plant 2 Weather Data ---\")\n",
    "\n",
    "# 1. Identify Unique Sensors\n",
    "p2_sensors = p2_weather['SOURCE_KEY'].unique()\n",
    "p2_sensor_count = len(p2_sensors)\n",
    "print(f\"1. Unique Sensors identified:   {p2_sensor_count}\")\n",
    "\n",
    "# 2. Expected Readings\n",
    "p2_expected = EXPECTED_PER_SENSOR * p2_sensor_count\n",
    "print(f\"2. Expected Readings (Target):  {p2_expected:,}\")\n",
    "\n",
    "# 3. Actual Readings\n",
    "p2_actual = len(p2_weather)\n",
    "print(f\"3. Actual Readings (Raw):       {p2_actual:,}\")\n",
    "\n",
    "# 4. Conclusion\n",
    "diff_p2 = p2_actual - p2_expected\n",
    "if diff_p2 == 0:\n",
    "    print(f\"4. Conclusion:                  Exact Match (Perfect Data)\")\n",
    "else:\n",
    "    print(f\"4. Conclusion:                  {diff_p2:+,} Rows \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ba560f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             DATE_TIME  PLANT_ID       SOURCE_KEY    DC_POWER    AC_POWER  \\\n",
      "0  2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "1  2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "2  2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "3  2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "4  2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "5  2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "6  2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "7  2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "8  2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "9  2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "10 2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "11 2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "12 2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "13 2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "14 2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "15 2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "16 2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "17 2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "18 2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "19 2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "20 2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "21 2020-05-16 09:00:00   4135001  ih0vzX44oOqAx2f  636.357143  623.414286   \n",
      "\n",
      "    DAILY_YIELD  TOTAL_YIELD  day Operating_Condition  \n",
      "0    869.142857  6192229.143   16          Suboptimal  \n",
      "1    869.142857  6192229.143   16          Suboptimal  \n",
      "2    869.142857  6192229.143   16          Suboptimal  \n",
      "3    869.142857  6192229.143   16          Suboptimal  \n",
      "4    869.142857  6192229.143   16          Suboptimal  \n",
      "5    869.142857  6192229.143   16          Suboptimal  \n",
      "6    869.142857  6192229.143   16          Suboptimal  \n",
      "7    869.142857  6192229.143   16          Suboptimal  \n",
      "8    869.142857  6192229.143   16          Suboptimal  \n",
      "9    869.142857  6192229.143   16          Suboptimal  \n",
      "10   869.142857  6192229.143   16          Suboptimal  \n",
      "11   869.142857  6192229.143   16          Suboptimal  \n",
      "12   869.142857  6192229.143   16          Suboptimal  \n",
      "13   869.142857  6192229.143   16          Suboptimal  \n",
      "14   869.142857  6192229.143   16          Suboptimal  \n",
      "15   869.142857  6192229.143   16          Suboptimal  \n",
      "16   869.142857  6192229.143   16          Suboptimal  \n",
      "17   869.142857  6192229.143   16          Suboptimal  \n",
      "18   869.142857  6192229.143   16             Optimal  \n",
      "19   869.142857  6192229.143   16          Suboptimal  \n",
      "20   869.142857  6192229.143   16          Suboptimal  \n",
      "21   869.142857  6192229.143   16          Suboptimal  \n"
     ]
    }
   ],
   "source": [
    "# Slice and do a visual inspection of the data to understand duplications\n",
    "\n",
    "# Define \"Primary Keys\" to make a row unique.\n",
    "key_columns = ['DATE_TIME', 'DC_POWER', 'SOURCE_KEY']\n",
    "\n",
    "# Find rows with duplicate Keys AND NOT duplicate Data\n",
    "has_duplicate_keys = plant1_gen.duplicated(subset=key_columns, keep=False)\n",
    "is_exact_duplicate = plant1_gen.duplicated(keep=False)\n",
    "\n",
    "# We only want rows where keys match, but data differs\n",
    "conflict_mask = has_duplicate_keys & ~is_exact_duplicate\n",
    "\n",
    "# Grab 1 random row to act as search parameter\n",
    "target_row = plant1_gen[conflict_mask].sample(1)\n",
    "\n",
    "# Retrieve the target row and its conflicts\n",
    "result = plant1_gen.merge(target_row[key_columns], on=key_columns)\n",
    "\n",
    "# Display result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d525f5",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "- When slicing data, we observe exact time stamps with all variables equal except for \"operating_condition\". Understanding this in detail and cleaning accordingly will be critical for the \"Classification of Operating Conditions\" task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "752738ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant 1 Generation - Data Types Report:\n",
      "                column    pandas_dtype  python_types\n",
      "0            DATE_TIME  datetime64[ns]   [Timestamp]\n",
      "1             PLANT_ID           int64         [int]\n",
      "2           SOURCE_KEY          object         [str]\n",
      "3             DC_POWER         float64       [float]\n",
      "4             AC_POWER         float64       [float]\n",
      "5          DAILY_YIELD         float64       [float]\n",
      "6          TOTAL_YIELD         float64       [float]\n",
      "7                  day           int64         [int]\n",
      "8  Operating_Condition          object  [str, float] \n",
      "\n",
      "Plant 1 Weather - Data Types Report:\n",
      "                column    pandas_dtype python_types\n",
      "0            DATE_TIME  datetime64[ns]  [Timestamp]\n",
      "1             PLANT_ID           int64        [int]\n",
      "2           SOURCE_KEY          object        [str]\n",
      "3  AMBIENT_TEMPERATURE         float64      [float]\n",
      "4   MODULE_TEMPERATURE         float64      [float]\n",
      "5          IRRADIATION         float64      [float] \n",
      "\n",
      "Plant 2 Generation - Data Types Report:\n",
      "                column    pandas_dtype python_types\n",
      "0            DATE_TIME  datetime64[ns]  [Timestamp]\n",
      "1             PLANT_ID           int64        [int]\n",
      "2           SOURCE_KEY          object        [str]\n",
      "3             DC_POWER         float64      [float]\n",
      "4             AC_POWER         float64      [float]\n",
      "5          DAILY_YIELD         float64      [float]\n",
      "6          TOTAL_YIELD         float64      [float]\n",
      "7  Operating_Condition          object        [str] \n",
      "\n",
      "Plant 2 Weather - Data Types Report:\n",
      "                column    pandas_dtype python_types\n",
      "0            DATE_TIME  datetime64[ns]  [Timestamp]\n",
      "1             PLANT_ID           int64        [int]\n",
      "2           SOURCE_KEY          object        [str]\n",
      "3  AMBIENT_TEMPERATURE         float64      [float]\n",
      "4   MODULE_TEMPERATURE         float64      [float]\n",
      "5          IRRADIATION         float64      [float]\n"
     ]
    }
   ],
   "source": [
    "## Check data types\n",
    "def get_types_report(df):\n",
    "    \"\"\"Generate a report of pandas dtypes and unique Python types for each column in the DataFrame.\"\"\"\n",
    "    types = {}\n",
    "    for col in df.columns:\n",
    "        # Get unique python types in the column (useful for spotting mixed types)\n",
    "        py_types = df[col].map(lambda x: type(x).__name__).unique().tolist()\n",
    "        types[col] = py_types\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "        'column': list(types.keys()),\n",
    "        'pandas_dtype': [df[col].dtype for col in types.keys()],\n",
    "        'python_types': [types[col] for col in types.keys()]\n",
    "    })\n",
    "    return result_df\n",
    "\n",
    "# Get unique Python types and pandas dtypes for the four datasets\n",
    "print(\"Plant 1 Generation - Data Types Report:\")\n",
    "print(get_types_report(plant1_gen), \"\\n\")\n",
    "\n",
    "print(\"Plant 1 Weather - Data Types Report:\")\n",
    "print(get_types_report(plant1_weather), \"\\n\")\n",
    "\n",
    "print(\"Plant 2 Generation - Data Types Report:\")\n",
    "print(get_types_report(plant2_gen), \"\\n\")\n",
    "\n",
    "print(\"Plant 2 Weather - Data Types Report:\")\n",
    "print(get_types_report(plant2_weather))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1a2fb",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "- Date formats: The variable \"DATE_TIME\" will be used to merge the data sets, however Currently, a type mismatch exists. \"Plant 1 - Generation data\" is stored in datetime format, while the other three are objects. Standarization will be required. Standardization is required not only to enable merging but to prevent temporal misalignment, specifically avoiding the parsing error where day/month swaps cause the 34-day timeline to erroneously appear as spanning 11 months\n",
    "\n",
    "- Mixed formats: In \"Plant 1 - Generation data\" the variable \"Operating_Condition\" mariable exhibits mixed data types, strings and float. This means there are probably missing values in this column (NaN)\n",
    "\n",
    "- Integers for categorical information: Variable Plant_ID is currently encoded as an integer across, is shown as an integer. However, it serves as a unique categorical identifier rather than a quantitative metric. This distinction must be explicitly noted during modeling to ensure algorithms do not misinterpret the ID as having numerical magnitude or order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5283b8",
   "metadata": {},
   "source": [
    "#### B. Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5ff3293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power Generation 1 - Missing Values Report:\n",
      "               Columns  Missing Values  Percentage Missing\n",
      "0            DATE_TIME               0                0.00\n",
      "1             PLANT_ID               0                0.00\n",
      "2           SOURCE_KEY               0                0.00\n",
      "3             DC_POWER               0                0.00\n",
      "4             AC_POWER               0                0.00\n",
      "5          DAILY_YIELD               0                0.00\n",
      "6          TOTAL_YIELD               0                0.00\n",
      "7                  day               0                0.00\n",
      "8  Operating_Condition           23098                2.26 \n",
      "\n",
      "Weather Sensor 1 - Missing Values Report:\n",
      "               Columns  Missing Values  Percentage Missing\n",
      "0            DATE_TIME               0                 0.0\n",
      "1             PLANT_ID               0                 0.0\n",
      "2           SOURCE_KEY               0                 0.0\n",
      "3  AMBIENT_TEMPERATURE               0                 0.0\n",
      "4   MODULE_TEMPERATURE               0                 0.0\n",
      "5          IRRADIATION               0                 0.0\n",
      "Power Generation 2 - Missing Values Report:\n",
      "               Columns  Missing Values  Percentage Missing\n",
      "0            DATE_TIME               0                 0.0\n",
      "1             PLANT_ID               0                 0.0\n",
      "2           SOURCE_KEY               0                 0.0\n",
      "3             DC_POWER               0                 0.0\n",
      "4             AC_POWER               0                 0.0\n",
      "5          DAILY_YIELD               0                 0.0\n",
      "6          TOTAL_YIELD               0                 0.0\n",
      "7  Operating_Condition               0                 0.0 \n",
      "\n",
      "Weather Sensor 2- Missing Values Report:\n",
      "               Columns  Missing Values  Percentage Missing\n",
      "0            DATE_TIME               0                 0.0\n",
      "1             PLANT_ID               0                 0.0\n",
      "2           SOURCE_KEY               0                 0.0\n",
      "3  AMBIENT_TEMPERATURE               0                 0.0\n",
      "4   MODULE_TEMPERATURE               0                 0.0\n",
      "5          IRRADIATION               0                 0.0\n"
     ]
    }
   ],
   "source": [
    "## Check for missing values in the four data sets\n",
    "\n",
    "def get_missing_data_report(df):\n",
    "    \"\"\"Generate a report of missing data percentages for each column in the DataFrame.\"\"\"\n",
    "    missing_data_report = pd.DataFrame({\n",
    "        'Columns': df.columns,\n",
    "        'Missing Values': df.isna().sum().values,\n",
    "        'Percentage Missing': ((df.isna().sum().values / len(df)) * 100).round(2)\n",
    "    })\n",
    "    return missing_data_report\n",
    "\n",
    "# Generate and print missing data report for the datasets\n",
    "print(\"Power Generation 1 - Missing Values Report:\")\n",
    "print(get_missing_data_report(plant1_gen),\"\\n\")\n",
    "print(\"Weather Sensor 1 - Missing Values Report:\")\n",
    "print(get_missing_data_report(plant1_weather))\n",
    "print(\"Power Generation 2 - Missing Values Report:\")\n",
    "print(get_missing_data_report(plant2_gen),\"\\n\")\n",
    "print(\"Weather Sensor 2- Missing Values Report:\")\n",
    "print(get_missing_data_report(plant2_weather))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d937ca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing 10426 mis-parsed dates in Plant 1 Gen...\n",
      "Power Generation 1 - Time Gap Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maria Sanchez\\AppData\\Local\\Temp\\ipykernel_11816\\3572626036.py:34: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  expected_range = pd.date_range(start=start, end=end, freq='15T')\n",
      "C:\\Users\\Maria Sanchez\\AppData\\Local\\Temp\\ipykernel_11816\\3572626036.py:34: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  expected_range = pd.date_range(start=start, end=end, freq='15T')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Metric                Value\n",
      "0          Start Time  2020-05-15 00:00:00\n",
      "1            End Time  2020-12-06 23:45:00\n",
      "2  Expected Intervals                19776\n",
      "3    Actual Intervals                 3158\n",
      "4   Missing Intervals                16618\n",
      "5           % Missing                84.03 \n",
      "\n",
      "Weather Sensor 1 - Time Gap Report:\n",
      "               Metric                Value\n",
      "0          Start Time  2020-05-15 00:00:00\n",
      "1            End Time  2020-06-17 23:45:00\n",
      "2  Expected Intervals                 3264\n",
      "3    Actual Intervals                 3182\n",
      "4   Missing Intervals                   82\n",
      "5           % Missing                 2.51 \n",
      "\n",
      "Power Generation 2 - Time Gap Report:\n",
      "               Metric                Value\n",
      "0          Start Time  2020-05-15 00:00:00\n",
      "1            End Time  2020-06-17 23:45:00\n",
      "2  Expected Intervals                 3264\n",
      "3    Actual Intervals                 3259\n",
      "4   Missing Intervals                    5\n",
      "5           % Missing                 0.15 \n",
      "\n",
      "Weather Sensor 2 - Time Gap Report:\n",
      "               Metric                Value\n",
      "0          Start Time  2020-05-15 00:00:00\n",
      "1            End Time  2020-06-17 23:45:00\n",
      "2  Expected Intervals                 3264\n",
      "3    Actual Intervals                 3259\n",
      "4   Missing Intervals                    5\n",
      "5           % Missing                 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maria Sanchez\\AppData\\Local\\Temp\\ipykernel_11816\\3572626036.py:34: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  expected_range = pd.date_range(start=start, end=end, freq='15T')\n",
      "C:\\Users\\Maria Sanchez\\AppData\\Local\\Temp\\ipykernel_11816\\3572626036.py:34: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  expected_range = pd.date_range(start=start, end=end, freq='15T')\n"
     ]
    }
   ],
   "source": [
    "## Check for missing data ranges (rows) in the four data sets\n",
    "\n",
    "# First we fix the Plan #1 - Generation dates\n",
    "\n",
    "# Convert to datetime (this creates the errors like Jan 6 or May 6)\n",
    "plant1_gen['DATE_TIME'] = pd.to_datetime(plant1_gen['DATE_TIME'])\n",
    "\n",
    "# Identify ALL bad dates (Anything before the known start of May 15th to avoid false positives)\n",
    "mask_bad_dates = plant1_gen['DATE_TIME'] < '2020-05-15'\n",
    "\n",
    "if mask_bad_dates.sum() > 0:\n",
    "    print(f\"Fixing {mask_bad_dates.sum()} mis-parsed dates in Plant 1 Gen...\")\n",
    "    # Swap Day and Month for these specific rows\n",
    "    plant1_gen.loc[mask_bad_dates, 'DATE_TIME'] = pd.to_datetime(\n",
    "        plant1_gen.loc[mask_bad_dates, 'DATE_TIME'].dt.strftime('%Y-%d-%m %H:%M:%S')\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Generate a report\n",
    "# -------------------------------------------------------\n",
    "def get_time_gap_report(df):\n",
    "    \"\"\"\n",
    "    Generate a report of missing time intervals in the DataFrame.\n",
    "    Assumes 'DATE_TIME' column exists.\n",
    "    \"\"\"\n",
    "    # Ensure datetime and sort\n",
    "    df = df.copy()\n",
    "    df['DATE_TIME'] = pd.to_datetime(df['DATE_TIME'])\n",
    "    df = df.sort_values('DATE_TIME')\n",
    "    \n",
    "    # Create the expected grid for the intervals (perfect 15 min intervals)\n",
    "    start = df['DATE_TIME'].min()\n",
    "    end = df['DATE_TIME'].max()\n",
    "    expected_range = pd.date_range(start=start, end=end, freq='15T')\n",
    "    \n",
    "    # Calculate statistics\n",
    "    expected_count = len(expected_range)\n",
    "    actual_count = df['DATE_TIME'].nunique()\n",
    "    missing_count = expected_count - actual_count\n",
    "    pct_missing = (missing_count / expected_count) * 100\n",
    "    \n",
    "    # Create Report DataFrame\n",
    "    report = pd.DataFrame({\n",
    "        'Metric': ['Start Time', 'End Time', 'Expected Intervals', 'Actual Intervals', 'Missing Intervals', '% Missing'],\n",
    "        'Value': [start, end, expected_count, actual_count, missing_count, round(pct_missing, 2)]\n",
    "    })\n",
    "    return report\n",
    "\n",
    "# Generate and print time gap reports\n",
    "print(\"Power Generation 1 - Time Gap Report:\")\n",
    "print(get_time_gap_report(plant1_gen),\"\\n\")\n",
    "\n",
    "print(\"Weather Sensor 1 - Time Gap Report:\")\n",
    "print(get_time_gap_report(plant1_weather),\"\\n\")\n",
    "\n",
    "print(\"Power Generation 2 - Time Gap Report:\")\n",
    "print(get_time_gap_report(plant2_gen),\"\\n\")\n",
    "\n",
    "print(\"Weather Sensor 2 - Time Gap Report:\")\n",
    "print(get_time_gap_report(plant2_weather))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f83843e",
   "metadata": {},
   "source": [
    "**Conclusion:** \n",
    "\n",
    "Missing values (NaN):\n",
    "- Plant 1: Generation data exhibits missing values. There are about 2.3% missing values in the Operating_Condition column of the data set. On the contrary, weather data shows 0 missing values (empty cells)\n",
    "- Plant 2: There are no missing values in the generation or weather data.\n",
    "\n",
    "Missing temporal dates (entire rows):\n",
    "- Plant 1: Generation data is missing 106 intervals (3.25%) and weather data is missing 82 intervals (2.51%).\n",
    "- Plant 2: The data set is nearly perfect, it only misses 5 intervals (0.15%) of information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f1b7d",
   "metadata": {},
   "source": [
    "#### C. Inconsistencies & Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a33de0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAFETY: Ensure all Time Columns are Datetime Objects ---\n",
    "# plant1_gen, was already fixed in the previous section\n",
    "plant1_weather['DATE_TIME'] = pd.to_datetime(plant1_weather['DATE_TIME'])\n",
    "plant2_gen['DATE_TIME'] = pd.to_datetime(plant2_gen['DATE_TIME'])\n",
    "plant2_weather['DATE_TIME'] = pd.to_datetime(plant2_weather['DATE_TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f2e8cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GENERATION CHECKS ---\n",
      "\n",
      "[Gen Check 1] Efficiency violation (AC > DC)\n",
      "  Plant 1: 396 rows failed.\n",
      "  Plant 2: 396 rows failed.\n",
      "\n",
      "[Gen Check 2] Negative power (AC or DC < 0)\n",
      "  Plant 1: 0 rows failed.\n",
      "  Plant 2: 0 rows failed.\n",
      "\n",
      "[Gen Check 3] Yield logic break \n",
      "  Plant 1: 439 rows failed.\n",
      "  Plant 2: 1888 rows failed.\n",
      "\n",
      "--- WEATHER CHECKS ---\n",
      "\n",
      "[Weather Check 1] Night irradiance (Irradiation > 0 at Night)\n",
      "  Plant 1: 0 rows failed.\n",
      "  Plant 2: 24 rows failed.\n",
      "\n",
      "[Weather Check 2] Hot panel at night (Mod Temp > Amb Temp + 5°C w/ No Sun)\n",
      "  Plant 1: 0 rows failed.\n",
      "  Plant 2: 0 rows failed.\n",
      "\n",
      "[Weather Check 3] Broken sensors (Ambient Temp frozen for 1hr)\n",
      "  Plant 1: 0 frozen sequences found.\n",
      "  Plant 2: 0 frozen sequences found.\n"
     ]
    }
   ],
   "source": [
    "## Inconsistencies are checked for different situations for all data sets\n",
    "# Grouping for iteration\n",
    "gen_data = [(\"Plant 1\", plant1_gen), (\"Plant 2\", plant2_gen)]\n",
    "weather_data = [(\"Plant 1\", plant1_weather), (\"Plant 2\", plant2_weather)]\n",
    "\n",
    "\n",
    "## Part A: Generation data\n",
    "print(\"\\n--- GENERATION CHECKS ---\")\n",
    "\n",
    "# CHECK 1: Efficiency violation \n",
    "# Impossible for AC Output > DC Input (Thermodynamics)\n",
    "\n",
    "# Plant 1: DC is in 100W units. Divide by 10 to convert to kW. Plant 2 DC is in kW. AC is in kW.\n",
    "plant1_gen['DC_POWER'] = plant1_gen['DC_POWER'] / 10.0\n",
    "\n",
    "print(\"\\n[Gen Check 1] Efficiency violation (AC > DC)\")\n",
    "for name, df in gen_data:\n",
    "    # We allow a tiny buffer (0.1 kW) for sensor timing mismatch\n",
    "    errors = df[df['AC_POWER'] > df['DC_POWER'] + 0.1]\n",
    "    print(f\"  {name}: {len(errors)} rows failed.\")\n",
    "\n",
    "# CHECK 2: Negative power\n",
    "# Solar panels cannot consume power (cannot be negative)\n",
    "print(\"\\n[Gen Check 2] Negative power (AC or DC < 0)\")\n",
    "for name, df in gen_data:\n",
    "    errors = df[(df['AC_POWER'] < 0) | (df['DC_POWER'] < 0)]\n",
    "    print(f\"  {name}: {len(errors)} rows failed.\")\n",
    "\n",
    "# CHECK 3: Total daily yield logic\n",
    "# Daily Yield should strictly increase. It only drops if the inverter resets (usually midnight).\n",
    "print(\"\\n[Gen Check 3] Yield logic break \")\n",
    "for name, df in gen_data:\n",
    "    # Vital: Sort by Inverter and Time, otherwise different inverters mix up the data\n",
    "    df_sorted = df.sort_values(by=['SOURCE_KEY', 'DATE_TIME'])\n",
    "    \n",
    "    # Calculate the change in yield from the previous reading OF THE SAME INVERTER\n",
    "    df_sorted['yield_diff'] = df_sorted.groupby('SOURCE_KEY')['DAILY_YIELD'].diff()\n",
    "    \n",
    "    # Check if the day changed (Midnight reset is normal)\n",
    "    day_changed = df_sorted['DATE_TIME'].dt.date != df_sorted['DATE_TIME'].shift(1).dt.date\n",
    "    \n",
    "    # Flag: Yield dropped (negative diff) AND the day did NOT change\n",
    "    errors = df_sorted[(df_sorted['yield_diff'] < 0) & (~day_changed)]\n",
    "    print(f\"  {name}: {len(errors)} rows failed.\")\n",
    "\n",
    "## Part B: Weather data\n",
    "\n",
    "print(\"\\n--- WEATHER CHECKS ---\")\n",
    "\n",
    "# CHECK 1: No irradiance at night\n",
    "# Irradiance > 0 when it is dark (10 PM - 4 AM short range assuming its summer)\n",
    "print(\"\\n[Weather Check 1] Night irradiance (Irradiation > 0 at Night)\")\n",
    "for name, df in weather_data:\n",
    "    hour = df['DATE_TIME'].dt.hour\n",
    "    night_mask = (hour >= 22) | (hour < 4)\n",
    "    errors = df[night_mask & (df['IRRADIATION'] > 0)]\n",
    "    print(f\"  {name}: {len(errors)} rows failed.\")\n",
    "\n",
    "# CHECK 2: Panel being hot without irradiance at night\n",
    "# No Sun (0 Irr) but Module is significantly hotter than Ambient\n",
    "print(\"\\n[Weather Check 2] Hot panel at night (Mod Temp > Amb Temp + 5°C w/ No Sun)\")\n",
    "for name, df in weather_data:\n",
    "    # If Irradiance is 0, Module shouldn't be hot compared to air\n",
    "    errors = df[(df['IRRADIATION'] == 0) & \n",
    "                (df['MODULE_TEMPERATURE'] > df['AMBIENT_TEMPERATURE'] + 5.0)]\n",
    "    print(f\"  {name}: {len(errors)} rows failed.\")\n",
    "\n",
    "# CHECK 3: Broken sensors\n",
    "# Ambient Temp stays exactly the same for 4 consecutive readings (1 hour). Testing for a extreme case.\n",
    "print(\"\\n[Weather Check 3] Broken sensors (Ambient Temp frozen for 1hr)\")\n",
    "for name, df in weather_data:\n",
    "    df_sorted = df.sort_values(by='DATE_TIME') \n",
    "    \n",
    "    # True if current value equals previous value\n",
    "    is_frozen = df_sorted['AMBIENT_TEMPERATURE'].diff() == 0\n",
    "    \n",
    "    # Rolling sum: if 4 consecutive rows are \"True\", we have a 1-hour freeze\n",
    "    frozen_blocks = is_frozen.rolling(4).sum() == 4\n",
    "    print(f\"  {name}: {frozen_blocks.sum()} frozen sequences found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d338d78",
   "metadata": {},
   "source": [
    "**Conclusion:** \n",
    "\n",
    "Generation data\n",
    "- Plant 1: There are 396 instances (~0.6%) of efficiency violations (system reports that AC output is higher than DC, which is physically imposible). Also, yield logic fails at 439 instances (~0.6%).\n",
    "- Plant 2: There are 396 instances (~0.6%) of efficiency violations (system reports that AC output is higher than DC, which is physically imposible). Also, yield logic fails at 1,888 instances (~2.7%). This high failure rate indicates severe data logger instability causing frequent mid-day resets for some.\n",
    "\n",
    "Weather data\n",
    "- Plant 1: Passed all consistency checks\n",
    "- Plant 2: 24 instances (~0.7%) of \"Night Sun\" (Irradiance > 0 at night) This culd be due to the sunrise/sunset exact timings or callibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c7f95",
   "metadata": {},
   "source": [
    "### 2.2.2 Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b25ebc",
   "metadata": {},
   "source": [
    "From the previous analysis, we would like to execute the following updates:\n",
    "\n",
    "A. General data type standardization\n",
    "- Date parsing (converting all information to date_time appropriately)\n",
    "- Numeric coercion (in preparation for imputation of values)\n",
    "- Categorical encoding\n",
    "- Scaling updates (Power Unit Scaling: Normalize AC and DC power units)\n",
    "\n",
    "B. Temporal alignment & completeness\n",
    "- Grid re-indexing (creating a master temporal grid of 15 minute frequency to ensure full coverage)\n",
    "\n",
    "C. Missing values imputation\n",
    "- Linear interpolation to address the missing data\n",
    "\n",
    "D. Anomaly correction\n",
    "- Efficiency Validation ($AC > DC$)\n",
    "- Cumulative Yield Reconstruction\n",
    "-Irradiance Clamping (\"Night Sun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "147f0c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded. '_clean' dataframes initialized.\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize Clean Dataframes ---\n",
    "plant1_gen_clean = plant1_gen.copy()\n",
    "plant1_weather_clean = plant1_weather.copy()\n",
    "plant2_gen_clean = plant2_gen.copy()\n",
    "plant2_weather_clean = plant2_weather.copy()\n",
    "\n",
    "print(\"Data Loaded. '_clean' dataframes initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3715842a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing 10426 mis-parsed dates in Plant 1 Gen...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maria Sanchez\\AppData\\Local\\Temp\\ipykernel_11816\\1601625078.py:18: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  plant1_weather_clean['DATE_TIME'] = pd.to_datetime(plant1_weather_clean['DATE_TIME'], dayfirst=True)\n",
      "C:\\Users\\Maria Sanchez\\AppData\\Local\\Temp\\ipykernel_11816\\1601625078.py:19: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  plant2_gen_clean['DATE_TIME'] = pd.to_datetime(plant2_gen_clean['DATE_TIME'], dayfirst=True)\n",
      "C:\\Users\\Maria Sanchez\\AppData\\Local\\Temp\\ipykernel_11816\\1601625078.py:20: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  plant2_weather_clean['DATE_TIME'] = pd.to_datetime(plant2_weather_clean['DATE_TIME'], dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Standardization & Scaling Complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. General data type standardization ---\n",
    "\n",
    "# Initial conversion for Plant 1 Gen\n",
    "plant1_gen_clean['DATE_TIME'] = pd.to_datetime(plant1_gen_clean['DATE_TIME'])\n",
    "\n",
    "# CUSTOM FIX: Identify mis-parsed dates (e.g. June 1st read as Jan 6th)\n",
    "# Logic: We know data starts May 15th. Any date before that is a parsing error.\n",
    "mask_bad_dates = plant1_gen_clean['DATE_TIME'] < '2020-05-15'\n",
    "\n",
    "if mask_bad_dates.sum() > 0:\n",
    "    print(f\"Fixing {mask_bad_dates.sum()} mis-parsed dates in Plant 1 Gen...\")\n",
    "    # Swap Day and Month for these specific rows\n",
    "    plant1_gen_clean.loc[mask_bad_dates, 'DATE_TIME'] = pd.to_datetime(\n",
    "        plant1_gen_clean.loc[mask_bad_dates, 'DATE_TIME'].dt.strftime('%Y-%d-%m %H:%M:%S')\n",
    "    )\n",
    "\n",
    "# Standard conversion for others (using dayfirst=True to prevent new errors)\n",
    "plant1_weather_clean['DATE_TIME'] = pd.to_datetime(plant1_weather_clean['DATE_TIME'], dayfirst=True)\n",
    "plant2_gen_clean['DATE_TIME'] = pd.to_datetime(plant2_gen_clean['DATE_TIME'], dayfirst=True)\n",
    "plant2_weather_clean['DATE_TIME'] = pd.to_datetime(plant2_weather_clean['DATE_TIME'], dayfirst=True)\n",
    "\n",
    "# --- 2. Numeric Coercion ---\n",
    "# Force 'Operating_Condition' to numeric; 'Suboptimal' strings become NaN\n",
    "plant1_gen_clean['Operating_Condition'] = pd.to_numeric(plant1_gen_clean['Operating_Condition'], errors='coerce')\n",
    "\n",
    "# --- 3. Categorical Encoding ---\n",
    "# Ensure PLANT_ID is a string label\n",
    "for df in [plant1_gen_clean, plant1_weather_clean, plant2_gen_clean, plant2_weather_clean]:\n",
    "    df['PLANT_ID'] = df['PLANT_ID'].astype(str)\n",
    "\n",
    "# --- 4. Power Unit Scaling ---\n",
    "# Plant 1 DC_POWER is scaled down by 10 to match Plant 2's magnitude (Watts vs kW check)\n",
    "plant1_gen_clean['DC_POWER'] = plant1_gen_clean['DC_POWER'] / 10\n",
    "\n",
    "print(\"Step 1: Standardization & Scaling Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150f4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cde041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55a94d98",
   "metadata": {},
   "source": [
    "### 2.2.3 Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c57ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bc752ad",
   "metadata": {},
   "source": [
    "## 2.3 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16aa47",
   "metadata": {},
   "source": [
    "### 2.3.1 Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7372e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a7952bd",
   "metadata": {},
   "source": [
    "### 2.3.2 Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c6200",
   "metadata": {},
   "source": [
    "### 2.3.3 Trend Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897be8f",
   "metadata": {},
   "source": [
    "### 2.3.4 Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba81ad",
   "metadata": {},
   "source": [
    "### 2.3.5 Pattern Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecbe1cd",
   "metadata": {},
   "source": [
    "## 2.4 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7549d692",
   "metadata": {},
   "source": [
    "### 2.4.1 Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4888e79",
   "metadata": {},
   "source": [
    "### 2.4.2 Feature Selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
