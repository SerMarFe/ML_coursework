{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b33985b",
   "metadata": {},
   "source": [
    "# Task 6 – Temporal Forecasting with LSTM\n",
    "\n",
    "In this task, we build a sequence model (LSTM) to forecast short-term solar power output using recent historical measurements.\n",
    "\n",
    "- **Target variable:** `DC_POWER`\n",
    "- **Model type:** Sequence-to-one time series forecasting (LSTM)\n",
    "- **Prediction horizon:** 1 hour ahead (4 × 15-minute steps)\n",
    "- **Input window:** Previous 24 hours of data (96 × 15-minute steps)\n",
    "\n",
    "We use the preprocessed dataset produced in the previous notebook (EDA & preprocessing), so we do not repeat merging or cleaning here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a9932",
   "metadata": {},
   "source": [
    "#### IMPORTS AND CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32909ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflowNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached tensorflow-2.20.0-cp311-cp311-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (6.33.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: pillow in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jccas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.20.0-cp311-cp311-win_amd64.whl (331.8 MB)\n",
      "Installing collected packages: tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\jccas\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\tensorflow\\\\include\\\\external\\\\com_github_grpc_grpc\\\\src\\\\core\\\\ext\\\\filters\\\\fault_injection\\\\fault_injection_service_config_parser.h'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\jccas\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5858f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d07aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration:\n",
      "  Lookback window: 96 timesteps (24 hours)\n",
      "  Forecast horizon: 4 timesteps (1 hour)\n",
      "  LSTM hidden units: 32\n",
      "  Dropout rate: 0.2\n"
     ]
    }
   ],
   "source": [
    "LOOKBACK = 96        # 24 hours * 4 readings per hour = 96 timesteps\n",
    "HORIZON = 4          # 1 hour ahead = 4 timesteps\n",
    "HIDDEN_SIZE = 32     # Number of LSTM units (16-64 recommended)\n",
    "DROPOUT_RATE = 0.2   # Regularization strength\n",
    "BATCH_SIZE = 32      # Training batch size\n",
    "MAX_EPOCHS = 50      # Maximum training epochs (early stopping will likely stop sooner)\n",
    "TRAIN_RATIO = 0.7    # 70% for training\n",
    "VAL_RATIO = 0.15     # 15% for validation, 15% for test\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Lookback window: {LOOKBACK} timesteps ({LOOKBACK/4:.0f} hours)\")\n",
    "print(f\"  Forecast horizon: {HORIZON} timesteps ({HORIZON/4:.0f} hour)\")\n",
    "print(f\"  LSTM hidden units: {HIDDEN_SIZE}\")\n",
    "print(f\"  Dropout rate: {DROPOUT_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e30ac5",
   "metadata": {},
   "source": [
    "#### DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e274da43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE_TIME</th>\n",
       "      <th>PLANT_ID</th>\n",
       "      <th>SOURCE_KEY</th>\n",
       "      <th>DC_POWER</th>\n",
       "      <th>AC_POWER</th>\n",
       "      <th>DAILY_YIELD</th>\n",
       "      <th>TOTAL_YIELD</th>\n",
       "      <th>Operating_Condition</th>\n",
       "      <th>AMBIENT_TEMPERATURE</th>\n",
       "      <th>IRRADIATION</th>\n",
       "      <th>MODULE_TEMPERATURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>4135001</td>\n",
       "      <td>1BY6WEcLGh8j5v7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6259559.0</td>\n",
       "      <td>Suboptimal</td>\n",
       "      <td>25.184316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.857507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>4135001</td>\n",
       "      <td>1IF53ai7Xc0U56Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6183645.0</td>\n",
       "      <td>Suboptimal</td>\n",
       "      <td>25.184316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.857507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>4135001</td>\n",
       "      <td>3PZuoBAID5Wc2HD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6987759.0</td>\n",
       "      <td>Suboptimal</td>\n",
       "      <td>25.184316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.857507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>4135001</td>\n",
       "      <td>7JYdWkrLSPkdwr4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7602960.0</td>\n",
       "      <td>Suboptimal</td>\n",
       "      <td>25.184316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.857507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>4135001</td>\n",
       "      <td>McdE0feGgRqW7Ca</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7158964.0</td>\n",
       "      <td>Suboptimal</td>\n",
       "      <td>25.184316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.857507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DATE_TIME  PLANT_ID       SOURCE_KEY  DC_POWER  AC_POWER  DAILY_YIELD  \\\n",
       "0 2020-05-15   4135001  1BY6WEcLGh8j5v7       0.0       0.0          0.0   \n",
       "1 2020-05-15   4135001  1IF53ai7Xc0U56Y       0.0       0.0          0.0   \n",
       "2 2020-05-15   4135001  3PZuoBAID5Wc2HD       0.0       0.0          0.0   \n",
       "3 2020-05-15   4135001  7JYdWkrLSPkdwr4       0.0       0.0          0.0   \n",
       "4 2020-05-15   4135001  McdE0feGgRqW7Ca       0.0       0.0          0.0   \n",
       "\n",
       "   TOTAL_YIELD Operating_Condition  AMBIENT_TEMPERATURE  IRRADIATION  \\\n",
       "0    6259559.0          Suboptimal            25.184316          0.0   \n",
       "1    6183645.0          Suboptimal            25.184316          0.0   \n",
       "2    6987759.0          Suboptimal            25.184316          0.0   \n",
       "3    7602960.0          Suboptimal            25.184316          0.0   \n",
       "4    7158964.0          Suboptimal            25.184316          0.0   \n",
       "\n",
       "   MODULE_TEMPERATURE  \n",
       "0           22.857507  \n",
       "1           22.857507  \n",
       "2           22.857507  \n",
       "3           22.857507  \n",
       "4           22.857507  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"../data/processed/plants_combined_data.csv\", parse_dates=[\"DATE_TIME\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7693ffcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available inverters: 44\n",
      "Using inverter: 1BY6WEcLGh8j5v7\n",
      "Inverter data: 3,154 rows\n",
      "Time range: 2020-05-15 00:00:00 to 2020-06-17 23:45:00\n"
     ]
    }
   ],
   "source": [
    "# Get list of inverters - check which column name exists\n",
    "if 'SOURCE_KEY' in df.columns:\n",
    "    inverter_col = 'SOURCE_KEY'\n",
    "elif 'SOURCE_KEY_gen' in df.columns:\n",
    "    inverter_col = 'SOURCE_KEY_gen'\n",
    "else:\n",
    "    print(\"ERROR: Cannot find inverter column (SOURCE_KEY or SOURCE_KEY_gen)\")\n",
    "    exit(1)\n",
    "\n",
    "inverters = df[inverter_col].unique()\n",
    "print(f\"\\nAvailable inverters: {len(inverters)}\")\n",
    "\n",
    "# Select first inverter (you can change this)\n",
    "selected_inverter = inverters[0]\n",
    "print(f\"Using inverter: {selected_inverter}\")\n",
    "\n",
    "# Filter data for selected inverter\n",
    "df_inv = df[df[inverter_col] == selected_inverter].copy()\n",
    "df_inv = df_inv.sort_values('DATE_TIME').reset_index(drop=True)\n",
    "\n",
    "print(f\"Inverter data: {len(df_inv):,} rows\")\n",
    "print(f\"Time range: {df_inv['DATE_TIME'].min()} to {df_inv['DATE_TIME'].max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a52aaa7",
   "metadata": {},
   "source": [
    "#### SEQUENCE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd3b0662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting DC_POWER for sequence creation...\n",
      "  Data shape: (3154, 1)\n",
      "  Min power: 0.00 kW\n",
      "  Max power: 1333.51 kW\n",
      "  Mean power: 287.37 kW\n",
      "\n",
      "Creating sequences with lookback=96, horizon=4...\n",
      " Created 3,054 sequences\n",
      "  Input shape (X): (3054, 96, 1)  # (n_sequences, lookback, 1)\n",
      "  Target shape (y): (3054, 4, 1)  # (n_sequences, horizon)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(data, lookback=96, horizon=4):\n",
    "    \"\"\"\n",
    "    Create sliding window sequences for LSTM training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like, shape (n_samples, n_features)\n",
    "        Time series data (e.g., power readings)\n",
    "    lookback : int\n",
    "        Number of past timesteps to use as input\n",
    "    horizon : int\n",
    "        Number of future timesteps to predict\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : array, shape (n_sequences, lookback, n_features)\n",
    "        Input sequences (past data)\n",
    "    y : array, shape (n_sequences, horizon)\n",
    "        Target sequences (future data to predict)\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    If lookback=96 and horizon=4:\n",
    "    X[0] = data[0:96]    → y[0] = data[96:100]   (timesteps 96-99)\n",
    "    X[1] = data[1:97]    → y[1] = data[97:101]   (timesteps 97-100)\n",
    "    X[2] = data[2:98]    → y[2] = data[98:102]   (timesteps 98-101)\n",
    "    ...\n",
    "    \"\"\"\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(lookback, len(data) - horizon):\n",
    "        # Get past lookback timesteps as input\n",
    "        X.append(data[i-lookback:i])\n",
    "        \n",
    "        # Get next horizon timesteps as target\n",
    "        y.append(data[i:i+horizon])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# Extract target variable (DC_POWER)\n",
    "# Check which column name exists\n",
    "if 'DC_POWER' in df_inv.columns:\n",
    "    power_column = 'DC_POWER'\n",
    "elif 'DC_POWER_gen' in df_inv.columns:\n",
    "    power_column = 'DC_POWER_gen'\n",
    "else:\n",
    "    print(\"ERROR: Cannot find power column (DC_POWER or DC_POWER_gen)\")\n",
    "    exit(1)\n",
    "\n",
    "power_data = df_inv[power_column].values.reshape(-1, 1)\n",
    "\n",
    "print(f\"Extracting {power_column} for sequence creation...\")\n",
    "print(f\"  Data shape: {power_data.shape}\")\n",
    "print(f\"  Min power: {power_data.min():.2f} kW\")\n",
    "print(f\"  Max power: {power_data.max():.2f} kW\")\n",
    "print(f\"  Mean power: {power_data.mean():.2f} kW\")\n",
    "\n",
    "# Create sequences\n",
    "print(f\"\\nCreating sequences with lookback={LOOKBACK}, horizon={HORIZON}...\")\n",
    "X, y = create_sequences(power_data, lookback=LOOKBACK, horizon=HORIZON)\n",
    "\n",
    "print(f\" Created {len(X):,} sequences\")\n",
    "print(f\"  Input shape (X): {X.shape}  # (n_sequences, lookback, 1)\")\n",
    "print(f\"  Target shape (y): {y.shape}  # (n_sequences, horizon)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04714ee0",
   "metadata": {},
   "source": [
    "#### MODEL TEMPORAL TRAINING, VALIDATION AND SPLIT TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eeaf692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split results:\n",
      "  Train: 2,137 sequences (70.0%)\n",
      "  Val:   458 sequences (15.0%)\n",
      "  Test:  459 sequences (15.0%)\n"
     ]
    }
   ],
   "source": [
    "def temporal_split(X, y, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Split sequences preserving temporal order.\n",
    "    \n",
    "    CRITICAL: NO SHUFFLING! Must respect time order to avoid data leakage.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    \n",
    "    train_end = int(n_samples * train_ratio)\n",
    "    val_end = int(n_samples * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Split chronologically\n",
    "    X_train, y_train = X[:train_end], y[:train_end]\n",
    "    X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
    "    X_test, y_test = X[val_end:], y[val_end:]\n",
    "    \n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "\n",
    "# Perform temporal split\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = temporal_split(\n",
    "    X, y, \n",
    "    train_ratio=TRAIN_RATIO, \n",
    "    val_ratio=VAL_RATIO\n",
    ")\n",
    "\n",
    "print(f\"Split results:\")\n",
    "print(f\"  Train: {len(X_train):,} sequences ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(X_val):,} sequences ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(X_test):,} sequences ({len(X_test)/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5441e9e",
   "metadata": {},
   "source": [
    "#### FEATURE SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d201897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting scaler on training data...\n",
      " Data scaled to range [0, 1]\n",
      "  Original range: [0.00, 1333.51]\n",
      "  Scaled range: [0.00, 1.00]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Reshape for scaling: (samples × timesteps, features) \n",
    "n_samples_train, n_steps, n_features = X_train.shape\n",
    "X_train_2d = X_train.reshape(-1, n_features)\n",
    "X_val_2d = X_val.reshape(-1, n_features)\n",
    "X_test_2d = X_test.reshape(-1, n_features)\n",
    "\n",
    "# Fit scaler on training data only\n",
    "print(\"Fitting scaler on training data...\")\n",
    "scaler.fit(X_train_2d)\n",
    "\n",
    "# Transform all sets\n",
    "X_train_scaled = scaler.transform(X_train_2d).reshape(X_train.shape)\n",
    "X_val_scaled = scaler.transform(X_val_2d).reshape(X_val.shape)\n",
    "X_test_scaled = scaler.transform(X_test_2d).reshape(X_test.shape)\n",
    "\n",
    "# Scale targets too\n",
    "y_train_scaled = scaler.transform(y_train.reshape(-1, 1)).reshape(y_train.shape)\n",
    "y_val_scaled = scaler.transform(y_val.reshape(-1, 1)).reshape(y_val.shape)\n",
    "y_test_scaled = scaler.transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "\n",
    "print(f\" Data scaled to range [0, 1]\")\n",
    "print(f\"  Original range: [{X_train.min():.2f}, {X_train.max():.2f}]\")\n",
    "print(f\"  Scaled range: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b23b184",
   "metadata": {},
   "source": [
    "#### MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e21ba72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LSTM model with 32 hidden units...\n",
      "WARNING:tensorflow:From C:\\Users\\jccas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\rnn\\lstm.py:148: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jccas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "\n",
      "Model Architecture:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 32)                4352      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4484 (17.52 KB)\n",
      "Trainable params: 4484 (17.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Total trainable parameters: 4,484\n"
     ]
    }
   ],
   "source": [
    "def build_lstm_model(lookback, n_features, hidden_size, horizon, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Build a simple LSTM model for power forecasting.\n",
    "    \n",
    "    Architecture:\n",
    "    1. LSTM layer (hidden_size units)\n",
    "    2. Dropout layer (regularization)\n",
    "    3. Dense output layer (horizon units)\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        # LSTM layer\n",
    "        LSTM(hidden_size, \n",
    "             input_shape=(lookback, n_features),\n",
    "             return_sequences=False),\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        # Dense output layer\n",
    "        Dense(horizon)\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model\n",
    "print(f\"Building LSTM model with {HIDDEN_SIZE} hidden units...\")\n",
    "model = build_lstm_model(\n",
    "    lookback=LOOKBACK,\n",
    "    n_features=1,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    horizon=HORIZON,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nTotal trainable parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e5059c",
   "metadata": {},
   "source": [
    "#### TRAIN LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52b22918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Max epochs: 50\n",
      "  Batch size: 32\n",
      "  Early stopping patience: 5 epochs\n",
      "  Validation metric: val_loss\n",
      "\n",
      "Starting training...\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From C:\\Users\\jccas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jccas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.0573 - mae: 0.1669\n",
      "Epoch 1: val_loss improved from inf to 0.01505, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 4s 32ms/step - loss: 0.0573 - mae: 0.1669 - val_loss: 0.0150 - val_mae: 0.0968\n",
      "Epoch 2/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0240 - mae: 0.1122\n",
      "Epoch 2: val_loss improved from 0.01505 to 0.01300, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 2s 23ms/step - loss: 0.0240 - mae: 0.1121 - val_loss: 0.0130 - val_mae: 0.0888\n",
      "Epoch 3/50\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.0211 - mae: 0.1035\n",
      "Epoch 3: val_loss improved from 0.01300 to 0.01121, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 22ms/step - loss: 0.0209 - mae: 0.1031 - val_loss: 0.0112 - val_mae: 0.0807\n",
      "Epoch 4/50\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.0194 - mae: 0.0967\n",
      "Epoch 4: val_loss improved from 0.01121 to 0.01010, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 0.0195 - mae: 0.0969 - val_loss: 0.0101 - val_mae: 0.0748\n",
      "Epoch 5/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0177 - mae: 0.0910\n",
      "Epoch 5: val_loss improved from 0.01010 to 0.00985, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 19ms/step - loss: 0.0178 - mae: 0.0912 - val_loss: 0.0099 - val_mae: 0.0737\n",
      "Epoch 6/50\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.0173 - mae: 0.0880\n",
      "Epoch 6: val_loss improved from 0.00985 to 0.00884, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 0.0173 - mae: 0.0879 - val_loss: 0.0088 - val_mae: 0.0663\n",
      "Epoch 7/50\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.0162 - mae: 0.0845\n",
      "Epoch 7: val_loss improved from 0.00884 to 0.00872, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 0.0160 - mae: 0.0838 - val_loss: 0.0087 - val_mae: 0.0596\n",
      "Epoch 8/50\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.0159 - mae: 0.0834\n",
      "Epoch 8: val_loss improved from 0.00872 to 0.00865, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 0.0158 - mae: 0.0831 - val_loss: 0.0087 - val_mae: 0.0645\n",
      "Epoch 9/50\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.0153 - mae: 0.0799\n",
      "Epoch 9: val_loss did not improve from 0.00865\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 0.0154 - mae: 0.0803 - val_loss: 0.0090 - val_mae: 0.0679\n",
      "Epoch 10/50\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.0158 - mae: 0.0814\n",
      "Epoch 10: val_loss improved from 0.00865 to 0.00795, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 0.0158 - mae: 0.0814 - val_loss: 0.0080 - val_mae: 0.0614\n",
      "Epoch 11/50\n",
      "64/67 [===========================>..] - ETA: 0s - loss: 0.0148 - mae: 0.0764\n",
      "Epoch 11: val_loss improved from 0.00795 to 0.00761, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 0.0148 - mae: 0.0768 - val_loss: 0.0076 - val_mae: 0.0588\n",
      "Epoch 12/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0144 - mae: 0.0775\n",
      "Epoch 12: val_loss improved from 0.00761 to 0.00760, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 0.0144 - mae: 0.0778 - val_loss: 0.0076 - val_mae: 0.0586\n",
      "Epoch 13/50\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.0145 - mae: 0.0759\n",
      "Epoch 13: val_loss improved from 0.00760 to 0.00758, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 0.0146 - mae: 0.0764 - val_loss: 0.0076 - val_mae: 0.0600\n",
      "Epoch 14/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0143 - mae: 0.0758\n",
      "Epoch 14: val_loss improved from 0.00758 to 0.00749, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 0.0143 - mae: 0.0759 - val_loss: 0.0075 - val_mae: 0.0561\n",
      "Epoch 15/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0138 - mae: 0.0740\n",
      "Epoch 15: val_loss improved from 0.00749 to 0.00725, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 0.0138 - mae: 0.0738 - val_loss: 0.0072 - val_mae: 0.0537\n",
      "Epoch 16/50\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.0135 - mae: 0.0728\n",
      "Epoch 16: val_loss did not improve from 0.00725\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 0.0135 - mae: 0.0728 - val_loss: 0.0073 - val_mae: 0.0562\n",
      "Epoch 17/50\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.0133 - mae: 0.0726\n",
      "Epoch 17: val_loss improved from 0.00725 to 0.00674, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 0.0132 - mae: 0.0724 - val_loss: 0.0067 - val_mae: 0.0505\n",
      "Epoch 18/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0711\n",
      "Epoch 18: val_loss did not improve from 0.00674\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 0.0131 - mae: 0.0712 - val_loss: 0.0070 - val_mae: 0.0532\n",
      "Epoch 19/50\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.0132 - mae: 0.0726\n",
      "Epoch 19: val_loss did not improve from 0.00674\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 0.0132 - mae: 0.0724 - val_loss: 0.0069 - val_mae: 0.0552\n",
      "Epoch 20/50\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.0134 - mae: 0.0723\n",
      "Epoch 20: val_loss did not improve from 0.00674\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 0.0133 - mae: 0.0719 - val_loss: 0.0068 - val_mae: 0.0508\n",
      "Epoch 21/50\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.0120 - mae: 0.0695\n",
      "Epoch 21: val_loss did not improve from 0.00674\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 0.0122 - mae: 0.0699 - val_loss: 0.0093 - val_mae: 0.0627\n",
      "Epoch 22/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0129 - mae: 0.0717\n",
      "Epoch 22: val_loss improved from 0.00674 to 0.00658, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 18ms/step - loss: 0.0129 - mae: 0.0719 - val_loss: 0.0066 - val_mae: 0.0510\n",
      "Epoch 23/50\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.0128 - mae: 0.0710\n",
      "Epoch 23: val_loss did not improve from 0.00658\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 0.0131 - mae: 0.0714 - val_loss: 0.0067 - val_mae: 0.0543\n",
      "Epoch 24/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0127 - mae: 0.0703\n",
      "Epoch 24: val_loss did not improve from 0.00658\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 0.0128 - mae: 0.0705 - val_loss: 0.0066 - val_mae: 0.0518\n",
      "Epoch 25/50\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.0123 - mae: 0.0689\n",
      "Epoch 25: val_loss did not improve from 0.00658\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 0.0122 - mae: 0.0686 - val_loss: 0.0068 - val_mae: 0.0555\n",
      "Epoch 26/50\n",
      "64/67 [===========================>..] - ETA: 0s - loss: 0.0119 - mae: 0.0678\n",
      "Epoch 26: val_loss improved from 0.00658 to 0.00636, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 0.0118 - mae: 0.0678 - val_loss: 0.0064 - val_mae: 0.0516\n",
      "Epoch 27/50\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.0121 - mae: 0.0679\n",
      "Epoch 27: val_loss did not improve from 0.00636\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 0.0120 - mae: 0.0678 - val_loss: 0.0069 - val_mae: 0.0571\n",
      "Epoch 28/50\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.0118 - mae: 0.0678\n",
      "Epoch 28: val_loss improved from 0.00636 to 0.00632, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 14ms/step - loss: 0.0118 - mae: 0.0678 - val_loss: 0.0063 - val_mae: 0.0505\n",
      "Epoch 29/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0121 - mae: 0.0692\n",
      "Epoch 29: val_loss did not improve from 0.00632\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 0.0121 - mae: 0.0691 - val_loss: 0.0064 - val_mae: 0.0536\n",
      "Epoch 30/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0117 - mae: 0.0673\n",
      "Epoch 30: val_loss improved from 0.00632 to 0.00625, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 0.0116 - mae: 0.0671 - val_loss: 0.0062 - val_mae: 0.0483\n",
      "Epoch 31/50\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.0116 - mae: 0.0672\n",
      "Epoch 31: val_loss did not improve from 0.00625\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 0.0116 - mae: 0.0672 - val_loss: 0.0063 - val_mae: 0.0483\n",
      "Epoch 32/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0116 - mae: 0.0672\n",
      "Epoch 32: val_loss improved from 0.00625 to 0.00599, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 17ms/step - loss: 0.0116 - mae: 0.0672 - val_loss: 0.0060 - val_mae: 0.0481\n",
      "Epoch 33/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0113 - mae: 0.0669\n",
      "Epoch 33: val_loss did not improve from 0.00599\n",
      "67/67 [==============================] - 1s 14ms/step - loss: 0.0113 - mae: 0.0669 - val_loss: 0.0060 - val_mae: 0.0482\n",
      "Epoch 34/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0114 - mae: 0.0662\n",
      "Epoch 34: val_loss improved from 0.00599 to 0.00593, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 0.0114 - mae: 0.0662 - val_loss: 0.0059 - val_mae: 0.0498\n",
      "Epoch 35/50\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.0115 - mae: 0.0672\n",
      "Epoch 35: val_loss improved from 0.00593 to 0.00580, saving model to ..\\data\\processed\\lstm_best_model.h5\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 0.0115 - mae: 0.0672 - val_loss: 0.0058 - val_mae: 0.0469\n",
      "Epoch 36/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0117 - mae: 0.0666\n",
      "Epoch 36: val_loss did not improve from 0.00580\n",
      "67/67 [==============================] - 1s 14ms/step - loss: 0.0116 - mae: 0.0666 - val_loss: 0.0059 - val_mae: 0.0476\n",
      "Epoch 37/50\n",
      "65/67 [============================>.] - ETA: 0s - loss: 0.0116 - mae: 0.0679\n",
      "Epoch 37: val_loss did not improve from 0.00580\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 0.0116 - mae: 0.0679 - val_loss: 0.0060 - val_mae: 0.0508\n",
      "Epoch 38/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0115 - mae: 0.0664\n",
      "Epoch 38: val_loss did not improve from 0.00580\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 0.0116 - mae: 0.0665 - val_loss: 0.0058 - val_mae: 0.0476\n",
      "Epoch 39/50\n",
      "64/67 [===========================>..] - ETA: 0s - loss: 0.0116 - mae: 0.0671\n",
      "Epoch 39: val_loss did not improve from 0.00580\n",
      "67/67 [==============================] - 1s 16ms/step - loss: 0.0115 - mae: 0.0671 - val_loss: 0.0062 - val_mae: 0.0474\n",
      "Epoch 40/50\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.0114 - mae: 0.0665Restoring model weights from the end of the best epoch: 35.\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00580\n",
      "67/67 [==============================] - 1s 15ms/step - loss: 0.0114 - mae: 0.0665 - val_loss: 0.0061 - val_mae: 0.0468\n",
      "Epoch 40: early stopping\n",
      "\n",
      "✓ Training complete!\n",
      "  Best epoch: 35\n",
      "  Final train loss: 0.011350\n",
      "  Final val loss: 0.006127\n"
     ]
    }
   ],
   "source": [
    "# Create output directory for Task 6\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Set up callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        str(output_dir / 'lstm_best_model.h5'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Max epochs: {MAX_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Early stopping patience: 5 epochs\")\n",
    "print(f\"  Validation metric: val_loss\")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nStarting training...\")\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    validation_data=(X_val_scaled, y_val_scaled),\n",
    "    epochs=MAX_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Training complete!\")\n",
    "print(f\"  Best epoch: {np.argmin(history.history['val_loss']) + 1}\")\n",
    "print(f\"  Final train loss: {history.history['loss'][-1]:.6f}\")\n",
    "print(f\"  Final val loss: {history.history['val_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1ad7c2",
   "metadata": {},
   "source": [
    "#### BASELINE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21df771c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating baseline models on test set...\n",
      "\n",
      "Persistence Baseline:\n",
      "  RMSE: 167.96 kW\n",
      "  MAE:  90.64 kW\n",
      "\n",
      "Moving Average Baseline (window=4):\n",
      "  RMSE: 190.92 kW\n",
      "  MAE:  113.32 kW\n"
     ]
    }
   ],
   "source": [
    "def persistence_baseline(X_test, y_test):\n",
    "    \"\"\"Persistence model: Next hour will be like last observation\"\"\"\n",
    "    \n",
    "    last_values = X_test[:, -1, 0]\n",
    "    y_pred = np.repeat(last_values.reshape(-1, 1), y_test.shape[1], axis=1)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test.flatten(), y_pred.flatten()))\n",
    "    mae = mean_absolute_error(y_test.flatten(), y_pred.flatten())\n",
    "    \n",
    "    print(f\"Persistence Baseline:\")\n",
    "    print(f\"  RMSE: {rmse:.2f} kW\")\n",
    "    print(f\"  MAE:  {mae:.2f} kW\")\n",
    "    \n",
    "    return y_pred, rmse, mae\n",
    "\n",
    "\n",
    "def moving_average_baseline(X_test, y_test, window=4):\n",
    "    \"\"\"Moving average: Next hour = average of last N observations\"\"\"\n",
    "    \n",
    "    avg_values = np.mean(X_test[:, -window:, 0], axis=1)\n",
    "    y_pred = np.repeat(avg_values.reshape(-1, 1), y_test.shape[1], axis=1)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test.flatten(), y_pred.flatten()))\n",
    "    mae = mean_absolute_error(y_test.flatten(), y_pred.flatten())\n",
    "    \n",
    "    print(f\"\\nMoving Average Baseline (window={window}):\")\n",
    "    print(f\"  RMSE: {rmse:.2f} kW\")\n",
    "    print(f\"  MAE:  {mae:.2f} kW\")\n",
    "    \n",
    "    return y_pred, rmse, mae\n",
    "\n",
    "\n",
    "# Evaluate baselines\n",
    "print(\"Evaluating baseline models on test set...\\n\")\n",
    "y_pred_persistence, pers_rmse, pers_mae = persistence_baseline(X_test, y_test)\n",
    "y_pred_ma, ma_rmse, ma_mae = moving_average_baseline(X_test, y_test, window=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f13d64c",
   "metadata": {},
   "source": [
    "#### EVALUATING LSTM MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08edea40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
